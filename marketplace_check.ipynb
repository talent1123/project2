{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data put path\n",
    "- hdfs dfs -put market_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "import requests\n",
    "from haversine import haversine\n",
    "spark = spark = SparkSession.builder.appName(\"market\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Pop\n",
    "\n",
    "- /final_df/market_pop (SAVE PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Population_test = spark.read.format('csv').option('header', 'true').option(\"encoding\",\"CP949\").load('market_data/market_pop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Population_test.createOrReplaceTempView(\"Population_test\")\n",
    "Population = spark.sql(\"select `상권_코드_명`, `기준_년_코드`,`기준_분기_코드` ,`상권_코드`, sum(cast(`총_생활인구_수` as int)) as `총_생활인구_수` from Population_test where `기준_년_코드` between 2020 and 2021 group by `상권_코드_명`, `상권_코드`, `기준_년_코드`, `기준_분기_코드` order by `기준_년_코드`, `기준_분기_코드`, `상권_코드`\")\n",
    "Population.createOrReplaceTempView(\"Population\")\n",
    "# Population.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv(\"/final_df/market_pop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Population.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Population.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_20_Q1 = spark.sql(\"\"\"\n",
    "select *\n",
    "from Population\n",
    "where `기준_년_코드` = 2020 and `기준_분기_코드` = 1\n",
    "\"\"\")\n",
    "\n",
    "pop_TRDAR_NM = pop_20_Q1.select('상권_코드_명').rdd.map(lambda x:x[0]).collect() # \"TRDAR_NM\"\n",
    "pop_20_Q1_list = pop_20_Q1.select('총_생활인구_수').rdd.map(lambda x:x[0]).collect()\n",
    "\n",
    "##\n",
    "\n",
    "pop_20_Q2 = spark.sql(\"\"\"\n",
    "select *\n",
    "from Population\n",
    "where `기준_년_코드` = 2020 and `기준_분기_코드` = 2\n",
    "\"\"\")\n",
    "\n",
    "pop_20_Q2_list = pop_20_Q2.select('총_생활인구_수').rdd.map(lambda x:x[0]).collect()\n",
    "\n",
    "##\n",
    "\n",
    "pop_20_Q3 = spark.sql(\"\"\"\n",
    "select *\n",
    "from Population\n",
    "where `기준_년_코드` = 2020 and `기준_분기_코드` = 3\n",
    "\"\"\")\n",
    "\n",
    "pop_20_Q3_list = pop_20_Q3.select('총_생활인구_수').rdd.map(lambda x:x[0]).collect()\n",
    "\n",
    "##\n",
    "\n",
    "pop_20_Q4 = spark.sql(\"\"\"\n",
    "select *\n",
    "from Population\n",
    "where `기준_년_코드` = 2020 and `기준_분기_코드` = 4\n",
    "\"\"\")\n",
    "\n",
    "pop_20_Q4_list = pop_20_Q4.select('총_생활인구_수').rdd.map(lambda x:x[0]).collect()\n",
    "\n",
    "##\n",
    "\n",
    "pop_21_Q1 = spark.sql(\"\"\"\n",
    "select *\n",
    "from Population\n",
    "where `기준_년_코드` = 2021 and `기준_분기_코드` = 1\n",
    "\"\"\")\n",
    "\n",
    "pop_21_Q1_list = pop_21_Q1.select('총_생활인구_수').rdd.map(lambda x:x[0]).collect()\n",
    "\n",
    "##\n",
    "\n",
    "pop_21_Q2 = spark.sql(\"\"\"\n",
    "select *\n",
    "from Population\n",
    "where `기준_년_코드` = 2021 and `기준_분기_코드` = 2\n",
    "\"\"\")\n",
    "\n",
    "pop_21_Q2_list = pop_21_Q2.select('총_생활인구_수').rdd.map(lambda x:x[0]).collect()\n",
    "\n",
    "##\n",
    "\n",
    "pop_21_Q3 = spark.sql(\"\"\"\n",
    "select *\n",
    "from Population\n",
    "where `기준_년_코드` = 2021 and `기준_분기_코드` = 3\n",
    "\"\"\")\n",
    "\n",
    "pop_21_Q3_list = pop_21_Q3.select('총_생활인구_수').rdd.map(lambda x:x[0]).collect()\n",
    "\n",
    "##\n",
    "\n",
    "pop_21_Q4 = spark.sql(\"\"\"\n",
    "select *\n",
    "from Population\n",
    "where `기준_년_코드` = 2021 and `기준_분기_코드` = 4\n",
    "\"\"\")\n",
    "\n",
    "pop_21_Q4_list = pop_21_Q4.select('총_생활인구_수').rdd.map(lambda x:x[0]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketplace_pop_head = [\"TRDAR_NM\", \"market_pop_2020_Q1\", \"market_pop_2020_Q2\",\"market_pop_2020_Q3\",\"market_pop_2020_Q4\",\n",
    "\"market_pop_2021_Q1\",\"market_pop_2021_Q2\",\"market_pop_2021_Q3\",\"market_pop_2021_Q4\"]\n",
    "\n",
    "temp = []\n",
    "\n",
    "for j in range(len(pop_TRDAR_NM)):\n",
    "    rows = Row(pop_TRDAR_NM[j], pop_20_Q1_list[j], pop_20_Q2_list[j], pop_20_Q3_list[j], pop_20_Q4_list[j], pop_21_Q1_list[j], pop_21_Q2_list[j], pop_21_Q3_list[j], pop_21_Q4_list[j])\n",
    "    temp.append(rows)\n",
    "\n",
    "\n",
    "marketplace_pop = spark.createDataFrame(temp, marketplace_pop_head)\n",
    "marketplace_pop.createOrReplaceTempView(\"marketplace_pop\")\n",
    "# marketplace_pop.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv(\"/final_df/marketplace_pop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|                    TRDAR_NM|market_pop_2020_Q1|market_pop_2020_Q2|market_pop_2020_Q3|market_pop_2020_Q4|market_pop_2021_Q1|market_pop_2021_Q2|market_pop_2021_Q3|market_pop_2021_Q4|\n",
      "+----------------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|                 이북5도청사|            465487|            478069|            469107|            465742|            471523|            466269|            462698|            457018|\n",
      "|                독립문역 1번|           1629774|           1592925|           1612687|           1631381|           1606205|           1616722|           1627390|           1605779|\n",
      "|              세검정초등학교|           1371520|           1383282|           1331012|           1349960|           1343257|           1325317|           1178263|           1158981|\n",
      "|                대신고등학교|           3260433|           3203457|           3197275|           3236011|           3199067|           3130654|           3521313|           3724017|\n",
      "|                      세검정|           1229275|           1227610|           1167095|           1198632|           1183218|           1169071|            881100|            839116|\n",
      "|              부암동주민센터|            356610|            351255|            323980|            334133|            330575|            336728|            261984|            247844|\n",
      "|사직공원(한국사회과학도서관)|           3294928|           3234825|           3247599|           3276733|           3244852|           3201389|           3688113|           3809477|\n",
      "|배화여자대학교(박노수미술관)|           3721735|           3785610|           3732012|           3708621|           3698705|           3784444|           3571784|           3729527|\n",
      "|                  자하문터널|            427606|            417487|            379411|            390656|            386345|            392696|            301673|            290955|\n",
      "|                  평창동서측|           1112666|           1131076|           1099369|           1103958|           1103155|           1088627|            827328|            771707|\n",
      "|                청운초등학교|            208276|            242247|            276714|            294748|            295316|            298524|            273434|            261862|\n",
      "|                  성곡미술관|           2567793|           2536358|           2529832|           2508787|           2439770|           2505707|           2219101|           2305713|\n",
      "|            체부동홍종문가옥|           2270801|           2323154|           2270686|           2219806|           2113740|           2200062|           2081175|           2280040|\n",
      "|                경복고등학교|            789530|            802333|            824335|            857564|            889565|            898775|            999405|           1092329|\n",
      "|                청와대사랑채|           1221739|           1225624|           1215238|           1203338|           1195452|           1225810|           1388623|           1579316|\n",
      "|                  평창동동측|            163196|            168885|            174187|            178072|            179087|            164423|            222247|            225589|\n",
      "|                  정독도서관|            918741|            910754|            872303|            866149|            833181|            909773|            892569|            946063|\n",
      "|                중앙고등학교|           1297186|           1264210|           1230934|           1218256|           1158143|           1224959|           1141833|           1217622|\n",
      "|                      창덕궁|           1369211|           1300193|           1279713|           1265128|           1235977|           1370277|           1193481|           1344600|\n",
      "|            서울국제고등학교|           4073121|           4191177|           4295903|           4461826|           4297459|           4390763|           4235376|           4363914|\n",
      "+----------------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 03:01:22,953 INFO spark.SparkContext: Starting job: showString at <unknown>:0\n",
      "2022-12-10 03:01:22,953 INFO scheduler.DAGScheduler: Got job 513 (showString at <unknown>:0) with 1 output partitions\n",
      "2022-12-10 03:01:22,953 INFO scheduler.DAGScheduler: Final stage: ResultStage 824 (showString at <unknown>:0)\n",
      "2022-12-10 03:01:22,953 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-12-10 03:01:22,953 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-12-10 03:01:22,953 INFO scheduler.DAGScheduler: Submitting ResultStage 824 (MapPartitionsRDD[2430] at showString at <unknown>:0), which has no missing parents\n",
      "2022-12-10 03:01:22,954 INFO memory.MemoryStore: Block broadcast_873 stored as values in memory (estimated size 15.4 KiB, free 429.5 MiB)\n",
      "2022-12-10 03:01:22,954 INFO memory.MemoryStore: Block broadcast_873_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 429.5 MiB)\n",
      "2022-12-10 03:01:22,955 INFO storage.BlockManagerInfo: Added broadcast_873_piece0 in memory on 192.168.254.129:45683 (size: 7.2 KiB, free: 433.5 MiB)\n",
      "2022-12-10 03:01:22,955 INFO spark.SparkContext: Created broadcast 873 from broadcast at DAGScheduler.scala:1478\n",
      "2022-12-10 03:01:22,955 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 824 (MapPartitionsRDD[2430] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-12-10 03:01:22,955 INFO cluster.YarnScheduler: Adding task set 824.0 with 1 tasks resource profile 0\n",
      "2022-12-10 03:01:22,955 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 824.0 (TID 734) (ubuntu, executor 2, partition 0, PROCESS_LOCAL, 39631 bytes) taskResourceAssignments Map()\n",
      "2022-12-10 03:01:22,958 INFO storage.BlockManagerInfo: Added broadcast_873_piece0 in memory on ubuntu:41197 (size: 7.2 KiB, free: 433.6 MiB)\n",
      "2022-12-10 03:01:22,970 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 824.0 (TID 734) in 15 ms on ubuntu (executor 2) (1/1)\n",
      "2022-12-10 03:01:22,970 INFO cluster.YarnScheduler: Removed TaskSet 824.0, whose tasks have all completed, from pool \n",
      "2022-12-10 03:01:22,971 INFO scheduler.DAGScheduler: ResultStage 824 (showString at <unknown>:0) finished in 0.017 s\n",
      "2022-12-10 03:01:22,971 INFO scheduler.DAGScheduler: Job 513 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-12-10 03:01:22,971 INFO cluster.YarnScheduler: Killing all running tasks in stage 824: Stage finished\n",
      "2022-12-10 03:01:22,971 INFO scheduler.DAGScheduler: Job 513 finished: showString at <unknown>:0, took 0.017722 s\n"
     ]
    }
   ],
   "source": [
    "marketplace_pop.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Densitiy\n",
    "\n",
    "* /final_df/market_densitiy (SAVE PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#점포수_2020\n",
    "Store1 = spark.read.format('csv').option('header', 'true').option(\"encoding\",\"CP949\").load('market_data/market_Y_2020.csv')\n",
    "Store1.createOrReplaceTempView(\"Store1\")\n",
    "\n",
    "Store_2020 = spark.sql(\"select `기준_년_코드`, `기준_분기_코드`, `상권_코드`, `상권_코드_명`, sum(cast(`점포_수` as int)) as `상권별_점포수` from Store1 where `상권_구분_코드_명` = '골목상권' group by  `상권_코드`, `기준_년_코드`, `기준_분기_코드`, `상권_코드_명` order by `기준_년_코드`, `기준_분기_코드`, `상권_코드`\")\n",
    "Store_2020.createOrReplaceTempView(\"Store_2020\")\n",
    "\n",
    "#점포수_2021\n",
    "Store2 = spark.read.format('csv').option('header', 'true').option(\"encoding\",\"CP949\").load('market_data/market_Y_2021.csv')\n",
    "Store2.createOrReplaceTempView(\"Store2\")\n",
    "\n",
    "Store_2021 = spark.sql(\"select `기준_년_코드`, `기준_분기_코드`, `상권_코드`, `상권_코드_명`, sum(cast(`점포_수` as int)) as `상권별_점포수` from Store2 where `상권_구분_코드_명` = '골목상권' group by `상권_코드_명`, `상권_코드`, `기준_년_코드`, `기준_분기_코드` order by `기준_년_코드`, `기준_분기_코드`, `상권_코드`\")\n",
    "Store_2021.createOrReplaceTempView(\"Store_2021\")\n",
    "\n",
    "STORE_count = Store_2020.union(Store_2021)\n",
    "STORE_count.createOrReplaceTempView(\"STORE_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_count = spark.sql(\"select `상권_코드_명`, avg(`상권별_점포수`) from STORE_count group by 1\")\n",
    "store_count.createOrReplaceTempView(\"store_count\")\n",
    "\n",
    "Store_count_area = spark.read.format('csv').option('header', 'true').option(\"encoding\",\"utf-8\").load('market_data/maketplace_core.csv')\n",
    "Store_count_area.createOrReplaceTempView('Store_count_area')\n",
    "\n",
    "store_scale = spark.sql(\"select TRDAR_NM, avg(`면적`) as scale from Store_count_area group by 1\")\n",
    "store_scale.createOrReplaceTempView(\"store_scale\")\n",
    "\n",
    "store_count_scale = spark.sql(\"select TRDAR_NM, `avg(상권별_점포수)` , scale from store_count join store_scale where `상권_코드_명`=TRDAR_NM\")\n",
    "store_count_scale.createOrReplaceTempView(\"store_count_scale\")\n",
    "\n",
    "market_densitiy = spark.sql(\"select TRDAR_NM, round(scale/`avg(상권별_점포수)`,2) as densitiy from store_count_scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_densitiy.show()\n",
    "# market_densitiy.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv(\"/final_df/market_densitiy\")\n",
    "# rows = market_densitiy.count()\n",
    "# print(f\"DataFrame Rows count : {rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Land Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_2020_test = spark.read.format('csv').option('header', 'true').load('market_data/market_realestate_2020.csv')\n",
    "price_2020_test.createOrReplaceTempView('price_2020_test')\n",
    "price_2020 = spark.sql(\"select `계약년월`, `시군구`, `도로명`, `용도지역`,`전용/연면적(㎡)`,`거래금액(만원)` from price_2020_test\")\n",
    "price_2020.createOrReplaceTempView('price_2020')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Land_price_2020 = spark.sql(\"\"\"select `계약년월`, `시군구`, `도로명`, `용도지역`,\n",
    "(cast(replace(`거래금액(만원)`, ',','') as float) /cast(`전용/연면적(㎡)` as float)) as price_per_scale\n",
    "from price_2020\n",
    "where `용도지역` in ('일반상업', '근린상업', '유통상업', '중심상업')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_2021_test = spark.read.format('csv').option('header', 'true').load('market_data/market_realestate_2021.csv')\n",
    "price_2021_test.createOrReplaceTempView('price_2021_test')\n",
    "price_2021 = spark.sql(\"select `계약년월`, `시군구`, `도로명`, `용도지역`,`전용/연면적(㎡)`,`거래금액(만원)` from price_2021_test\")\n",
    "price_2021.createOrReplaceTempView('price_2021')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Land_price_2021 = spark.sql(\"\"\"select `계약년월`, `시군구`, `도로명`, `용도지역`,\n",
    "(cast(replace(`거래금액(만원)`, ',','') as float) /cast(`전용/연면적(㎡)` as float)) as price_per_scale\n",
    "from price_2021\n",
    "where `용도지역` in ('일반상업', '근린상업', '유통상업', '중심상업')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Land_price_total = Land_price_2020.union(Land_price_2021)\n",
    "Land_price_total.createOrReplaceTempView(\"Land_price_total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Land_price_total.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test20 = Land_price_2020.select('시군구').rdd.map(lambda x:x[0]).collect() # 주소 포함\n",
    "test21 = Land_price_2021.select('시군구').rdd.map(lambda x:x[0]).collect() # 주소 포함\n",
    "testtt = Land_price_total.select('시군구').rdd.map(lambda x:x[0]).collect() # 주소 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(testtt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import parse\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "from urllib.error import HTTPError\n",
    "import json\n",
    "\n",
    "market_place_coor ={}\n",
    "for i in testtt:\n",
    "    Client_ID = \"0dahjnik2r\"\n",
    "    Client_Secret=\"laJOj2DMSCX29PCCXMY6thLXVHtavtomJ1qDQT6t\"\n",
    "    api_url = 'https://naveropenapi.apigw.ntruss.com/map-geocode/v2/geocode?query='\n",
    "    test = i # 주소는 2번째 위치 >> [1]\n",
    "    try:\n",
    "        add_urlenc = parse.quote(test) # URL Encoding\n",
    "    except:\n",
    "        # trick. 변환 불가할 경우 0,0 좌표를 줘서 1km 필터에 걸리지 않도록 처리\n",
    "        latitude, longitude = 0, 0\n",
    "    print(add_urlenc)\n",
    "    url = api_url + add_urlenc\n",
    "    request = Request(url)   \n",
    "    request.add_header('X-NCP-APIGW-API-KEY-ID', Client_ID)\n",
    "    request.add_header('X-NCP-APIGW-API-KEY', Client_Secret)\n",
    "\n",
    "    try:\n",
    "        response = urlopen(request)\n",
    "\n",
    "    except HTTPError as e:\n",
    "        print('HTTP Error')\n",
    "        latitude, longitude = 0, 0\n",
    "\n",
    "    else:\n",
    "        rescode = response.getcode()\n",
    "        \n",
    "        \n",
    "        if rescode == 200:\n",
    "            coordset = []\n",
    "            response_body = response.read().decode('utf-8')\n",
    "            response_body = json.loads(response_body)\n",
    "            \n",
    "            if response_body['addresses'] == []:\n",
    "                coordset.append(0)\n",
    "                coordset.append(0)\n",
    "                market_place_coor[test]=coordset\n",
    "            else:\n",
    "                longitude = float(response_body['addresses'][0]['x'])\n",
    "                latitude = float(response_body['addresses'][0]['y'])\n",
    "                print('Success')\n",
    "                print(longitude, latitude)\n",
    "                coordset.append(longitude)\n",
    "                coordset.append(latitude)\n",
    "                market_place_coor[test]=coordset\n",
    "        else:\n",
    "            print(f'Response error, rescode:{rescode}')\n",
    "            latitude, longitude = 0, 0\n",
    "            coordset.append(latitude)\n",
    "            coordset.append(longitude)\n",
    "            market_place_coor[test]=coordset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_addr_coor = []\n",
    "for i in testtt:\n",
    "    temp=[]\n",
    "    temp.append(i)\n",
    "    temp.append(market_place_coor[i])\n",
    "    land_addr_coor.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(market_place_coor[testtt[11240]])\n",
    "print((land_addr_coor[11240]))\n",
    "print(len(land_addr_coor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(land_addr_coor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_addr_coor_sort = []\n",
    "for i in land_addr_coor:\n",
    "    if i not in land_addr_coor_sort:\n",
    "        land_addr_coor_sort.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_addr_coor_index = ['시군구','market_coor_X', 'market_coor_Y']\n",
    "temp = []\n",
    "for i in range(len(land_addr_coor_sort)):\n",
    "    count = Row(land_addr_coor_sort[i][0], float(land_addr_coor_sort[i][1][0]), float(land_addr_coor_sort[i][1][1]))\n",
    "    temp.append(count)\n",
    "land_addr_coor_sort_df = spark.createDataFrame(temp, land_addr_coor_index)\n",
    "# land_addr_coor_sort_df.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv(\"/final_df/land_addr_coor_sort_df\")\n",
    "land_addr_coor_sort_df.createOrReplaceTempView(\"land_addr_coor_sort_df\")\n",
    "land_addr_coor_sort_sgg = land_addr_coor_sort_df.select('시군구').rdd.map(lambda x:x[0]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(land_addr_coor_sort_sgg))\n",
    "land_addr_coor_sort_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# land_addr_coor_index = ['시군구','market_coor_X', 'market_coor_Y']\n",
    "# temp = []\n",
    "# for i in range(len(land_addr_coor)):\n",
    "#     count = Row(land_addr_coor[i][0], float(land_addr_coor[i][1][0]), float(land_addr_coor[i][1][1]))\n",
    "#     temp.append(count)\n",
    "# land_addr_coor_df = spark.createDataFrame(temp, land_addr_coor_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# land_addr_coor_df.createOrReplaceTempView(\"land_addr_coor_df\")\n",
    "# land_addr_coor_sgg = land_addr_coor_df.select('시군구').rdd.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(land_addr_coor_sgg))\n",
    "# land_addr_coor_df.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv(\"/final_df/land_addr_coor_sgg\")\n",
    "# 조회 시 크롤링 원치 않으면 본 파일 불러서 실행하면 됨\n",
    "land_addr_coor_df = spark.read.format('csv').option('header', 'true').option(\"encoding\",\"utf-8\").load('/final_df/Land_price_total')\n",
    "land_addr_coor_df.createOrReplaceTempView(\"land_addr_coor_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Land_price_total.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv(\"/final_df/Land_price_total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_addr_coor_sort_df.printSchema()\n",
    "Land_price_total.printSchema()\n",
    "# where 1 between to_date('2020-01-01', 'YYYY-MM-DD') and to_date ('2020-03-31', 'YYYY-MM-DD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_addr_coor_final_df = spark.sql(\"\"\"select `계약년월`, a.`시군구`, market_coor_X, market_coor_Y, price_per_scale\n",
    "from land_addr_coor_sort_df a left outer join Land_price_total b on a.`시군구` = b.`시군구`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_addr_coor_final_df.createOrReplaceTempView(\"land_addr_coor_final_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_addr_coor_final_df.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv(\"/final_df/land_addr_coor_final_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate by quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load core data\n",
    "core = spark.read.format('csv').option('header', 'true').option(\"encoding\",\"utf-8\").load('market_data/maketplace_core.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core.printSchema()\n",
    "core_NM = core.select('TRDAR_NM').rdd.map(lambda x:x[0]).collect()\n",
    "core_X = core.select('X').rdd.map(lambda x:x[0]).collect()\n",
    "core_Y = core.select('Y').rdd.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_20_Q1 = spark.sql(\"\"\"\n",
    "select `시군구`, avg(market_coor_X) as X,avg(market_coor_Y) as Y ,avg(price_per_scale) as PPS\n",
    "from land_addr_coor_final_df\n",
    "where `계약년월` between 202001 and 202003\n",
    "group by `시군구`\n",
    "\"\"\")\n",
    "# table_20_Q1.printSchema()\n",
    "table_20_Q1_X = table_20_Q1.select('X').rdd.map(lambda x:x[0]).collect()\n",
    "table_20_Q1_Y = table_20_Q1.select('Y').rdd.map(lambda x:x[0]).collect()\n",
    "table_20_Q1_PPS = table_20_Q1.select('PPS').rdd.map(lambda x:x[0]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_20_res = []\n",
    "for core in range(len(core_Y)):\n",
    "    Q1_20 =[]\n",
    "    test_point1 = [float(core_Y[core]),float(core_X[core])]\n",
    "    Q1_20_temp = 500\n",
    "    Q1_20_price_index = 0\n",
    "    for table in range(len(table_20_Q1_Y)):\n",
    "        test_point2 = [float(table_20_Q1_Y[table]),float(table_20_Q1_X[table])]\n",
    "        dis_test =  haversine(test_point1, test_point2, unit=\"km\")\n",
    "        if dis_test < Q1_20_temp:\n",
    "            Q1_20_temp = dis_test\n",
    "            Q1_20_price_index = table\n",
    "    Q1_20.append(core_NM[core])\n",
    "    Q1_20.append(table_20_Q1_PPS[Q1_20_price_index])\n",
    "    Q1_20_res.append(Q1_20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_20_Q2 = spark.sql(\"\"\"\n",
    "select `시군구`, avg(market_coor_X) as X,avg(market_coor_Y) as Y ,avg(price_per_scale) as PPS\n",
    "from land_addr_coor_final_df\n",
    "where `계약년월` between 202001 and 202003\n",
    "group by `시군구`\n",
    "\"\"\")\n",
    "# table_20_Q2.printSchema()\n",
    "table_20_Q2_X = table_20_Q2.select('X').rdd.map(lambda x:x[0]).collect()\n",
    "table_20_Q2_Y = table_20_Q2.select('Y').rdd.map(lambda x:x[0]).collect()\n",
    "table_20_Q2_PPS = table_20_Q2.select('PPS').rdd.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2_20_res = []\n",
    "for core in range(len(core_Y)):\n",
    "    Q2_20 =[]\n",
    "    test_point1 = [float(core_Y[core]),float(core_X[core])]\n",
    "    Q2_20_temp = 500\n",
    "    Q2_20_price_index = 0\n",
    "    for table in range(len(table_20_Q2_Y)):\n",
    "        test_point2 = [float(table_20_Q2_Y[table]),float(table_20_Q2_X[table])]\n",
    "        dis_test =  haversine(test_point1, test_point2, unit=\"km\")\n",
    "        if dis_test < Q2_20_temp:\n",
    "            Q2_20_temp = dis_test\n",
    "            Q2_20_price_index = table\n",
    "    Q2_20.append(core_NM[core])\n",
    "    Q2_20.append(table_20_Q2_PPS[Q2_20_price_index])\n",
    "    Q2_20_res.append(Q2_20)\n",
    "print((Q2_20_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_20_Q3 = spark.sql(\"\"\"\n",
    "select `시군구`, avg(market_coor_X) as X,avg(market_coor_Y) as Y ,avg(price_per_scale) as PPS\n",
    "from land_addr_coor_final_df\n",
    "where `계약년월` between 202001 and 202003\n",
    "group by `시군구`\n",
    "\"\"\")\n",
    "# table_20_Q3.printSchema()\n",
    "table_20_Q3_X = table_20_Q3.select('X').rdd.map(lambda x:x[0]).collect()\n",
    "table_20_Q3_Y = table_20_Q3.select('Y').rdd.map(lambda x:x[0]).collect()\n",
    "table_20_Q3_PPS = table_20_Q3.select('PPS').rdd.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3_20_res = []\n",
    "for core in range(len(core_Y)):\n",
    "    Q3_20 =[]\n",
    "    test_point1 = [float(core_Y[core]),float(core_X[core])]\n",
    "    Q3_20_temp = 500\n",
    "    Q3_20_price_index = 0\n",
    "    for table in range(len(table_20_Q3_Y)):\n",
    "        test_point2 = [float(table_20_Q3_Y[table]),float(table_20_Q3_X[table])]\n",
    "        dis_test =  haversine(test_point1, test_point2, unit=\"km\")\n",
    "        if dis_test < Q3_20_temp:\n",
    "            Q3_20_temp = dis_test\n",
    "            Q3_20_price_index = table\n",
    "    Q3_20.append(core_NM[core])\n",
    "    Q3_20.append(table_20_Q3_PPS[Q3_20_price_index])\n",
    "    Q3_20_res.append(Q3_20)\n",
    "print((Q3_20_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_20_Q4 = spark.sql(\"\"\"\n",
    "select `시군구`, avg(market_coor_X) as X,avg(market_coor_Y) as Y ,avg(price_per_scale) as PPS\n",
    "from land_addr_coor_final_df\n",
    "where `계약년월` between 202001 and 202003\n",
    "group by `시군구`\n",
    "\"\"\")\n",
    "# table_20_Q4.printSchema()\n",
    "table_20_Q4_X = table_20_Q4.select('X').rdd.map(lambda x:x[0]).collect()\n",
    "table_20_Q4_Y = table_20_Q4.select('Y').rdd.map(lambda x:x[0]).collect()\n",
    "table_20_Q4_PPS = table_20_Q4.select('PPS').rdd.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4_20_res = []\n",
    "for core in range(len(core_Y)):\n",
    "    Q4_20 =[]\n",
    "    test_point1 = [float(core_Y[core]),float(core_X[core])]\n",
    "    Q4_20_temp = 500\n",
    "    Q4_20_price_index = 0\n",
    "    for table in range(len(table_20_Q4_Y)):\n",
    "        test_point2 = [float(table_20_Q4_Y[table]),float(table_20_Q4_X[table])]\n",
    "        dis_test =  haversine(test_point1, test_point2, unit=\"km\")\n",
    "        if dis_test < Q4_20_temp:\n",
    "            Q4_20_temp = dis_test\n",
    "            Q4_20_price_index = table\n",
    "    Q4_20.append(core_NM[core])\n",
    "    Q4_20.append(table_20_Q4_PPS[Q4_20_price_index])\n",
    "    Q4_20_res.append(Q4_20)\n",
    "print((Q4_20_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_21_Q1 = spark.sql(\"\"\"\n",
    "select `시군구`, avg(market_coor_X) as X,avg(market_coor_Y) as Y ,avg(price_per_scale) as PPS\n",
    "from land_addr_coor_final_df\n",
    "where `계약년월` between 202001 and 202003\n",
    "group by `시군구`\n",
    "\"\"\")\n",
    "# table_21_Q1.printSchema()\n",
    "table_21_Q1_X = table_21_Q1.select('X').rdd.map(lambda x:x[0]).collect()\n",
    "table_21_Q1_Y = table_21_Q1.select('Y').rdd.map(lambda x:x[0]).collect()\n",
    "table_21_Q1_PPS = table_21_Q1.select('PPS').rdd.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_21_res = []\n",
    "for core in range(len(core_Y)):\n",
    "    Q1_21 =[]\n",
    "    test_point1 = [float(core_Y[core]),float(core_X[core])]\n",
    "    Q1_21_temp = 500\n",
    "    Q1_21_price_index = 0\n",
    "    for table in range(len(table_21_Q1_Y)):\n",
    "        test_point2 = [float(table_21_Q1_Y[table]),float(table_21_Q1_X[table])]\n",
    "        dis_test =  haversine(test_point1, test_point2, unit=\"km\")\n",
    "        if dis_test < Q1_21_temp:\n",
    "            Q1_21_temp = dis_test\n",
    "            Q1_21_price_index = table\n",
    "    Q1_21.append(core_NM[core])\n",
    "    Q1_21.append(table_21_Q1_PPS[Q1_21_price_index])\n",
    "    Q1_21_res.append(Q1_21)\n",
    "print((Q1_21_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_21_Q2 = spark.sql(\"\"\"\n",
    "select `시군구`, avg(market_coor_X) as X,avg(market_coor_Y) as Y ,avg(price_per_scale) as PPS\n",
    "from land_addr_coor_final_df\n",
    "where `계약년월` between 202001 and 202003\n",
    "group by `시군구`\n",
    "\"\"\")\n",
    "# table_21_Q2.printSchema()\n",
    "table_21_Q2_X = table_21_Q2.select('X').rdd.map(lambda x:x[0]).collect()\n",
    "table_21_Q2_Y = table_21_Q2.select('Y').rdd.map(lambda x:x[0]).collect()\n",
    "table_21_Q2_PPS = table_21_Q2.select('PPS').rdd.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2_21_res = []\n",
    "for core in range(len(core_Y)):\n",
    "    Q2_21 =[]\n",
    "    test_point1 = [float(core_Y[core]),float(core_X[core])]\n",
    "    Q2_21_temp = 500\n",
    "    Q2_21_price_index = 0\n",
    "    for table in range(len(table_21_Q2_Y)):\n",
    "        test_point2 = [float(table_21_Q2_Y[table]),float(table_21_Q2_X[table])]\n",
    "        dis_test =  haversine(test_point1, test_point2, unit=\"km\")\n",
    "        if dis_test < Q2_21_temp:\n",
    "            Q2_21_temp = dis_test\n",
    "            Q2_21_price_index = table\n",
    "    Q2_21.append(core_NM[core])\n",
    "    Q2_21.append(table_21_Q2_PPS[Q2_21_price_index])\n",
    "    Q2_21_res.append(Q2_21)\n",
    "print((Q2_21_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_21_Q3 = spark.sql(\"\"\"\n",
    "select `시군구`, avg(market_coor_X) as X,avg(market_coor_Y) as Y ,avg(price_per_scale) as PPS\n",
    "from land_addr_coor_final_df\n",
    "where `계약년월` between 202001 and 202003\n",
    "group by `시군구`\n",
    "\"\"\")\n",
    "# table_21_Q3.printSchema()\n",
    "table_21_Q3_X = table_21_Q3.select('X').rdd.map(lambda x:x[0]).collect()\n",
    "table_21_Q3_Y = table_21_Q3.select('Y').rdd.map(lambda x:x[0]).collect()\n",
    "table_21_Q3_PPS = table_21_Q3.select('PPS').rdd.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3_21_res = []\n",
    "for core in range(len(core_Y)):\n",
    "    Q3_21 =[]\n",
    "    test_point1 = [float(core_Y[core]),float(core_X[core])]\n",
    "    Q3_21_temp = 500\n",
    "    Q3_21_price_index = 0\n",
    "    for table in range(len(table_21_Q3_Y)):\n",
    "        test_point2 = [float(table_21_Q3_Y[table]),float(table_21_Q3_X[table])]\n",
    "        dis_test =  haversine(test_point1, test_point2, unit=\"km\")\n",
    "        if dis_test < Q3_21_temp:\n",
    "            Q3_21_temp = dis_test\n",
    "            Q3_21_price_index = table\n",
    "    Q3_21.append(core_NM[core])\n",
    "    Q3_21.append(table_21_Q3_PPS[Q3_21_price_index])\n",
    "    Q3_21_res.append(Q3_21)\n",
    "print((Q3_21_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_21_Q4 = spark.sql(\"\"\"\n",
    "select `시군구`, avg(market_coor_X) as X,avg(market_coor_Y) as Y ,avg(price_per_scale) as PPS\n",
    "from land_addr_coor_final_df\n",
    "where `계약년월` between 202001 and 202003\n",
    "group by `시군구`\n",
    "\"\"\")\n",
    "# table_21_Q4.printSchema()\n",
    "table_21_Q4_X = table_21_Q4.select('X').rdd.map(lambda x:x[0]).collect()\n",
    "table_21_Q4_Y = table_21_Q4.select('Y').rdd.map(lambda x:x[0]).collect()\n",
    "table_21_Q4_PPS = table_21_Q4.select('PPS').rdd.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4_21_res = []\n",
    "for core in range(len(core_Y)):\n",
    "    Q4_21 =[]\n",
    "    test_point1 = [float(core_Y[core]),float(core_X[core])]\n",
    "    Q4_21_temp = 500\n",
    "    Q4_21_price_index = 0\n",
    "    for table in range(len(table_21_Q4_Y)):\n",
    "        test_point2 = [float(table_21_Q4_Y[table]),float(table_21_Q4_X[table])]\n",
    "        dis_test =  haversine(test_point1, test_point2, unit=\"km\")\n",
    "        if dis_test < Q4_21_temp:\n",
    "            Q4_21_temp = dis_test\n",
    "            Q4_21_price_index = table\n",
    "    Q4_21.append(core_NM[core])\n",
    "    Q4_21.append(table_21_Q4_PPS[Q4_21_price_index])\n",
    "    Q4_21_res.append(Q4_21)\n",
    "print((Q4_21_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_land_price_byQ = [Q1_20_res, Q2_20_res, Q3_20_res, Q4_20_res, Q1_21_res, Q2_21_res, Q3_21_res, Q4_21_res]\n",
    "for i in market_land_price_byQ:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(market_land_price_byQ[0])):\n",
    "    if core_NM[i] != market_land_price_byQ[7][i][0]:\n",
    "        print(\"alert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(core_NM))\n",
    "for i in market_land_price_byQ:\n",
    "    print (len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(market_land_price_byQ[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate land price dataframe with market by quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_price_quarter_head = [\"TRDAR_NM\", \"land_price_2020_Q1\", \"land_price_2020_Q2\",\"land_price_2020_Q3\",\"land_price_2020_Q4\",\n",
    "\"land_price_2021_Q1\",\"land_price_2021_Q2\",\"land_price_2021_Q3\",\"land_price_2021_Q4\"]\n",
    "\n",
    "temp = []\n",
    "\n",
    "for j in range(len(market_land_price_byQ[0])):\n",
    "    rows = Row(core_NM[j], market_land_price_byQ[0][j][1],market_land_price_byQ[1][j][1],market_land_price_byQ[2][j][1],market_land_price_byQ[3][j][1],market_land_price_byQ[4][j][1],market_land_price_byQ[5][j][1],\n",
    "    market_land_price_byQ[6][j][1],market_land_price_byQ[7][j][1])\n",
    "    temp.append(rows)\n",
    "\n",
    "\n",
    "land_price_final = spark.createDataFrame(temp, land_price_quarter_head)\n",
    "land_price_final.createOrReplaceTempView(\"land_price_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# land_price_final.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv(\"/final_df/land_price_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|              TRDAR_NM|land_price_2020_Q1|land_price_2020_Q2|land_price_2020_Q3|land_price_2020_Q4|land_price_2021_Q1|land_price_2021_Q2|land_price_2021_Q3|land_price_2021_Q4|\n",
      "+----------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|          난향초등학교|  720.968400367769| 628.9308326992499| 628.9308326992499| 918.6413034205582|1382.6266504066034| 918.6413034205582|  720.968400367769| 465.4758411164392|\n",
      "|      금강수목원아파트| 704.4987702408898| 720.1022755135147| 720.1022755135147| 965.9240893465774| 492.0686936033308| 965.9240893465774| 704.4987702408898| 465.4758411164392|\n",
      "|        답십리초등학교| 468.3237982534177| 673.7816708324573| 673.7816708324573|1346.3888307188886| 797.9650224170445|1346.3888307188886| 468.3237982534177| 673.7816708324573|\n",
      "|          청담초등학교| 3387.112047105866| 963.3537498211567| 963.3537498211567|1446.8535283327512| 476.3760697044681|1446.8535283327512| 3387.112047105866| 532.9256838463219|\n",
      "|            대방역 6번|  1877.46316562622|  545.818785367247|  545.818785367247|2224.5332153629392| 816.4948453608248|2224.5332153629392|  1877.46316562622| 421.7903848574354|\n",
      "|          신림초등학교|1203.5573429300698| 620.1550265338311| 620.1550265338311| 865.5471089943042|1396.9235945118837| 865.5471089943042|1203.5573429300698| 1554.494982570821|\n",
      "|          금천해태공원|  720.968400367769|  545.818785367247|  545.818785367247|2224.5332153629392|1382.6266504066034|2224.5332153629392|  720.968400367769| 465.4758411164392|\n",
      "|        미성동주민센터| 465.4758411164392|  545.818785367247|  545.818785367247|2224.5332153629392|1130.2340127653008|2224.5332153629392| 465.4758411164392| 465.4758411164392|\n",
      "|          백산초등학교|  720.968400367769|1382.6266504066034|1382.6266504066034|  720.968400367769|1382.6266504066034|  720.968400367769|  720.968400367769| 465.4758411164392|\n",
      "|     석수역 1번 출입구|  720.968400367769|1382.6266504066034|1382.6266504066034|  720.968400367769|1382.6266504066034|  720.968400367769|  720.968400367769| 953.2466655554388|\n",
      "|       독산2동주민센터| 767.4552282124864|  545.818785367247|  545.818785367247|2224.5332153629392| 1228.177752041734|2224.5332153629392| 767.4552282124864| 465.4758411164392|\n",
      "|          남천초등학교|  753.847342171196|   693.40483022291|   693.40483022291|1104.1322489270322|1506.1942514382804|1104.1322489270322|  753.847342171196|  288.083643237601|\n",
      "|          연신내역 7번|  545.818785367247|1203.5573429300698|1203.5573429300698|  288.083643237601|  86.4304257888049|  288.083643237601|  545.818785367247| 832.5492064330836|\n",
      "|내를건너서숲으로도서관| 832.5492064330836|3125.6750516556967|3125.6750516556967|  352.531827311358|394.17162037150536|  352.531827311358| 832.5492064330836| 659.5494388008146|\n",
      "|          장충초등학교| 4173.212419796624| 3573.306260157242| 3573.306260157242|2475.7537216209284| 565.7066085559861|2475.7537216209284| 4173.212419796624| 3573.306260157242|\n",
      "|           장충단 고개| 4173.212419796624| 3573.306260157242| 3573.306260157242|2475.7537216209284| 565.7066085559861|2475.7537216209284| 4173.212419796624| 3573.306260157242|\n",
      "|          신방화역 8번|   636.10038704817|   636.10038704817|   636.10038704817|1664.3743978552166| 4173.212419796624|1664.3743978552166|   636.10038704817|   636.10038704817|\n",
      "|  금호삼성래미안아파트| 565.7066085559861| 599.4435887480091| 599.4435887480091| 3531.940882973505| 788.5112164579414| 3531.940882973505| 565.7066085559861| 865.5471089943042|\n",
      "|          오류고등학교| 704.4987702408898| 720.1022755135147| 720.1022755135147| 965.9240893465774| 492.0686936033308| 965.9240893465774| 704.4987702408898|   636.10038704817|\n",
      "|      서울금융고등학교| 407.8680391814454| 720.1022755135147| 720.1022755135147| 965.9240893465774|  1524.03462805104| 965.9240893465774| 407.8680391814454|   636.10038704817|\n",
      "+----------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 03:01:57,430 INFO datasources.InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "2022-12-10 03:01:57,450 INFO storage.BlockManagerInfo: Removed broadcast_864_piece0 on ubuntu:41197 in memory (size: 30.5 KiB, free: 433.6 MiB)\n",
      "2022-12-10 03:01:57,450 INFO storage.BlockManagerInfo: Removed broadcast_864_piece0 on 192.168.254.129:45683 in memory (size: 30.5 KiB, free: 433.5 MiB)\n",
      "2022-12-10 03:01:57,452 INFO datasources.InMemoryFileIndex: It took 5 ms to list leaf files for 2 paths.\n",
      "2022-12-10 03:01:57,459 INFO storage.BlockManagerInfo: Removed broadcast_867_piece0 on 192.168.254.129:45683 in memory (size: 28.8 KiB, free: 433.6 MiB)\n",
      "2022-12-10 03:01:57,459 INFO storage.BlockManagerInfo: Removed broadcast_867_piece0 on ubuntu:41197 in memory (size: 28.8 KiB, free: 433.7 MiB)\n",
      "2022-12-10 03:01:57,460 INFO storage.BlockManagerInfo: Removed broadcast_867_piece0 on ubuntu:35935 in memory (size: 28.8 KiB, free: 433.6 MiB)\n",
      "2022-12-10 03:01:57,462 INFO storage.BlockManagerInfo: Removed broadcast_868_piece0 on 192.168.254.129:45683 in memory (size: 28.7 KiB, free: 433.6 MiB)\n",
      "2022-12-10 03:01:57,465 INFO storage.BlockManagerInfo: Removed broadcast_868_piece0 on ubuntu:41197 in memory (size: 28.7 KiB, free: 433.7 MiB)\n",
      "2022-12-10 03:01:57,469 INFO storage.BlockManagerInfo: Removed broadcast_870_piece0 on ubuntu:35935 in memory (size: 30.6 KiB, free: 433.7 MiB)\n",
      "2022-12-10 03:01:57,469 INFO storage.BlockManagerInfo: Removed broadcast_870_piece0 on 192.168.254.129:45683 in memory (size: 30.6 KiB, free: 433.6 MiB)\n",
      "2022-12-10 03:01:57,477 INFO storage.BlockManagerInfo: Removed broadcast_871_piece0 on 192.168.254.129:45683 in memory (size: 7.2 KiB, free: 433.6 MiB)\n",
      "2022-12-10 03:01:57,480 INFO storage.BlockManagerInfo: Removed broadcast_871_piece0 on ubuntu:41197 in memory (size: 7.2 KiB, free: 433.7 MiB)\n",
      "2022-12-10 03:01:57,484 INFO storage.BlockManagerInfo: Removed broadcast_865_piece0 on 192.168.254.129:45683 in memory (size: 30.6 KiB, free: 433.7 MiB)\n",
      "2022-12-10 03:01:57,485 INFO storage.BlockManagerInfo: Removed broadcast_865_piece0 on ubuntu:41197 in memory (size: 30.6 KiB, free: 433.7 MiB)\n",
      "2022-12-10 03:01:57,495 INFO storage.BlockManagerInfo: Removed broadcast_863_piece0 on ubuntu:35935 in memory (size: 28.7 KiB, free: 433.7 MiB)\n",
      "2022-12-10 03:01:57,501 INFO storage.BlockManagerInfo: Removed broadcast_863_piece0 on 192.168.254.129:45683 in memory (size: 28.7 KiB, free: 433.7 MiB)\n",
      "2022-12-10 03:01:57,503 INFO storage.BlockManagerInfo: Removed broadcast_872_piece0 on ubuntu:35935 in memory (size: 117.1 KiB, free: 433.8 MiB)\n",
      "2022-12-10 03:01:57,504 INFO storage.BlockManagerInfo: Removed broadcast_872_piece0 on 192.168.254.129:45683 in memory (size: 117.1 KiB, free: 433.8 MiB)\n",
      "2022-12-10 03:01:57,504 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-12-10 03:01:57,504 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#6113, None)) > 0)\n",
      "2022-12-10 03:01:57,504 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2022-12-10 03:01:57,505 INFO storage.BlockManagerInfo: Removed broadcast_872_piece0 on ubuntu:41197 in memory (size: 117.1 KiB, free: 433.8 MiB)\n",
      "2022-12-10 03:01:57,506 INFO memory.MemoryStore: Block broadcast_874 stored as values in memory (estimated size 303.0 KiB, free 430.3 MiB)\n",
      "2022-12-10 03:01:57,510 INFO storage.BlockManagerInfo: Removed broadcast_869_piece0 on ubuntu:35935 in memory (size: 30.5 KiB, free: 433.8 MiB)\n",
      "2022-12-10 03:01:57,512 INFO storage.BlockManagerInfo: Removed broadcast_869_piece0 on 192.168.254.129:45683 in memory (size: 30.5 KiB, free: 433.8 MiB)\n",
      "2022-12-10 03:01:57,512 INFO memory.MemoryStore: Block broadcast_874_piece0 stored as bytes in memory (estimated size 53.8 KiB, free 430.3 MiB)\n",
      "2022-12-10 03:01:57,512 INFO storage.BlockManagerInfo: Added broadcast_874_piece0 in memory on 192.168.254.129:45683 (size: 53.8 KiB, free: 433.8 MiB)\n",
      "2022-12-10 03:01:57,513 INFO spark.SparkContext: Created broadcast 874 from csv at NativeMethodAccessorImpl.java:0\n",
      "2022-12-10 03:01:57,513 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4285111 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-12-10 03:01:57,517 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "2022-12-10 03:01:57,518 INFO scheduler.DAGScheduler: Got job 514 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-12-10 03:01:57,518 INFO scheduler.DAGScheduler: Final stage: ResultStage 825 (csv at NativeMethodAccessorImpl.java:0)\n",
      "2022-12-10 03:01:57,518 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-12-10 03:01:57,518 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-12-10 03:01:57,518 INFO scheduler.DAGScheduler: Submitting ResultStage 825 (MapPartitionsRDD[2434] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-12-10 03:01:57,519 INFO memory.MemoryStore: Block broadcast_875 stored as values in memory (estimated size 11.6 KiB, free 430.4 MiB)\n",
      "2022-12-10 03:01:57,520 INFO memory.MemoryStore: Block broadcast_875_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 430.4 MiB)\n",
      "2022-12-10 03:01:57,520 INFO storage.BlockManagerInfo: Added broadcast_875_piece0 in memory on 192.168.254.129:45683 (size: 5.8 KiB, free: 433.8 MiB)\n",
      "2022-12-10 03:01:57,520 INFO spark.SparkContext: Created broadcast 875 from broadcast at DAGScheduler.scala:1478\n",
      "2022-12-10 03:01:57,520 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 825 (MapPartitionsRDD[2434] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-12-10 03:01:57,520 INFO cluster.YarnScheduler: Adding task set 825.0 with 1 tasks resource profile 0\n",
      "2022-12-10 03:01:57,521 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 825.0 (TID 735) (ubuntu, executor 1, partition 0, NODE_LOCAL, 4940 bytes) taskResourceAssignments Map()\n",
      "2022-12-10 03:01:57,522 INFO storage.BlockManagerInfo: Removed broadcast_873_piece0 on 192.168.254.129:45683 in memory (size: 7.2 KiB, free: 433.8 MiB)\n",
      "2022-12-10 03:01:57,522 INFO storage.BlockManagerInfo: Removed broadcast_873_piece0 on ubuntu:41197 in memory (size: 7.2 KiB, free: 433.8 MiB)\n",
      "2022-12-10 03:01:57,525 INFO storage.BlockManagerInfo: Added broadcast_875_piece0 in memory on ubuntu:35935 (size: 5.8 KiB, free: 433.8 MiB)\n",
      "2022-12-10 03:01:57,536 INFO storage.BlockManagerInfo: Added broadcast_874_piece0 in memory on ubuntu:35935 (size: 53.8 KiB, free: 433.8 MiB)\n",
      "2022-12-10 03:01:57,550 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 825.0 (TID 735) in 29 ms on ubuntu (executor 1) (1/1)\n",
      "2022-12-10 03:01:57,550 INFO cluster.YarnScheduler: Removed TaskSet 825.0, whose tasks have all completed, from pool \n",
      "2022-12-10 03:01:57,551 INFO scheduler.DAGScheduler: ResultStage 825 (csv at NativeMethodAccessorImpl.java:0) finished in 0.033 s\n",
      "2022-12-10 03:01:57,551 INFO scheduler.DAGScheduler: Job 514 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-12-10 03:01:57,551 INFO cluster.YarnScheduler: Killing all running tasks in stage 825: Stage finished\n",
      "2022-12-10 03:01:57,551 INFO scheduler.DAGScheduler: Job 514 finished: csv at NativeMethodAccessorImpl.java:0, took 0.033290 s\n",
      "2022-12-10 03:01:57,555 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-12-10 03:01:57,555 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-12-10 03:01:57,555 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2022-12-10 03:01:57,555 INFO memory.MemoryStore: Block broadcast_876 stored as values in memory (estimated size 303.0 KiB, free 430.1 MiB)\n",
      "2022-12-10 03:01:57,563 INFO memory.MemoryStore: Block broadcast_876_piece0 stored as bytes in memory (estimated size 53.8 KiB, free 430.1 MiB)\n",
      "2022-12-10 03:01:57,563 INFO storage.BlockManagerInfo: Added broadcast_876_piece0 in memory on 192.168.254.129:45683 (size: 53.8 KiB, free: 433.7 MiB)\n",
      "2022-12-10 03:01:57,563 INFO spark.SparkContext: Created broadcast 876 from csv at NativeMethodAccessorImpl.java:0\n",
      "2022-12-10 03:01:57,564 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4285111 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-12-10 03:01:57,573 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-12-10 03:01:57,573 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-12-10 03:01:57,573 INFO datasources.FileSourceStrategy: Output Data Schema: struct<TRDAR_NM: string, land_price_2020_Q1: string, land_price_2020_Q2: string, land_price_2020_Q3: string, land_price_2020_Q4: string ... 7 more fields>\n",
      "2022-12-10 03:01:57,576 INFO memory.MemoryStore: Block broadcast_877 stored as values in memory (estimated size 302.9 KiB, free 429.8 MiB)\n",
      "2022-12-10 03:01:57,581 INFO memory.MemoryStore: Block broadcast_877_piece0 stored as bytes in memory (estimated size 53.7 KiB, free 429.7 MiB)\n",
      "2022-12-10 03:01:57,581 INFO storage.BlockManagerInfo: Added broadcast_877_piece0 in memory on 192.168.254.129:45683 (size: 53.7 KiB, free: 433.7 MiB)\n",
      "2022-12-10 03:01:57,581 INFO spark.SparkContext: Created broadcast 877 from showString at <unknown>:0\n",
      "2022-12-10 03:01:57,582 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4285111 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-12-10 03:01:57,586 INFO spark.SparkContext: Starting job: showString at <unknown>:0\n",
      "2022-12-10 03:01:57,587 INFO scheduler.DAGScheduler: Got job 515 (showString at <unknown>:0) with 1 output partitions\n",
      "2022-12-10 03:01:57,587 INFO scheduler.DAGScheduler: Final stage: ResultStage 826 (showString at <unknown>:0)\n",
      "2022-12-10 03:01:57,587 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-12-10 03:01:57,587 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-12-10 03:01:57,587 INFO scheduler.DAGScheduler: Submitting ResultStage 826 (MapPartitionsRDD[2443] at showString at <unknown>:0), which has no missing parents\n",
      "2022-12-10 03:01:57,587 INFO memory.MemoryStore: Block broadcast_878 stored as values in memory (estimated size 10.8 KiB, free 429.7 MiB)\n",
      "2022-12-10 03:01:57,588 INFO memory.MemoryStore: Block broadcast_878_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 429.7 MiB)\n",
      "2022-12-10 03:01:57,588 INFO storage.BlockManagerInfo: Added broadcast_878_piece0 in memory on 192.168.254.129:45683 (size: 5.8 KiB, free: 433.7 MiB)\n",
      "2022-12-10 03:01:57,588 INFO spark.SparkContext: Created broadcast 878 from broadcast at DAGScheduler.scala:1478\n",
      "2022-12-10 03:01:57,588 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 826 (MapPartitionsRDD[2443] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-12-10 03:01:57,588 INFO cluster.YarnScheduler: Adding task set 826.0 with 1 tasks resource profile 0\n",
      "2022-12-10 03:01:57,588 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 826.0 (TID 736) (ubuntu, executor 1, partition 0, NODE_LOCAL, 4940 bytes) taskResourceAssignments Map()\n",
      "2022-12-10 03:01:57,593 INFO storage.BlockManagerInfo: Added broadcast_878_piece0 in memory on ubuntu:35935 (size: 5.8 KiB, free: 433.8 MiB)\n",
      "2022-12-10 03:01:57,596 INFO storage.BlockManagerInfo: Added broadcast_877_piece0 in memory on ubuntu:35935 (size: 53.7 KiB, free: 433.7 MiB)\n",
      "2022-12-10 03:01:57,608 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 826.0 (TID 736) in 20 ms on ubuntu (executor 1) (1/1)\n",
      "2022-12-10 03:01:57,608 INFO cluster.YarnScheduler: Removed TaskSet 826.0, whose tasks have all completed, from pool \n",
      "2022-12-10 03:01:57,608 INFO scheduler.DAGScheduler: ResultStage 826 (showString at <unknown>:0) finished in 0.021 s\n",
      "2022-12-10 03:01:57,608 INFO scheduler.DAGScheduler: Job 515 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-12-10 03:01:57,608 INFO cluster.YarnScheduler: Killing all running tasks in stage 826: Stage finished\n",
      "2022-12-10 03:01:57,608 INFO scheduler.DAGScheduler: Job 515 finished: showString at <unknown>:0, took 0.021731 s\n"
     ]
    }
   ],
   "source": [
    "load_test = spark.read.option(\"header\", \"true\").csv(\"/final_df/land_price_final\")\n",
    "load_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
